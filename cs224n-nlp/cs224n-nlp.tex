\documentclass[12pt]{scrartcl}
\usepackage[sexy]{james}
\usepackage[noend]{algpseudocode}
\usepackage{answers}
\usepackage{array}
\usepackage{tikz}
\newenvironment{allintypewriter}{\ttfamily}{\par}
\usepackage{listings}
\usepackage{xcolor}
\usetikzlibrary{arrows.meta}
\usepackage{color}
\usepackage{mathtools}
\newcommand{\U}{\mathcal{U}}
\newcommand{\E}{\mathbb{E}}
\usetikzlibrary{arrows}
\Newassociation{hint}{hintitem}{all-hints}
\renewcommand{\solutionextension}{out}
\renewenvironment{hintitem}[1]{\item[\bfseries #1.]}{}
\renewcommand{\O}{\mathcal{O}}
\declaretheorem[style=thmbluebox,name={Chinese Remainder Theorem}]{CRT}
\renewcommand{\theCRT}{\Alph{CRT}}
\setlength\parindent{0pt}
\usepackage{sansmath}
\usepackage{pgfplots}

\usetikzlibrary{automata}
\usetikzlibrary{positioning}  %                 ...positioning nodes
\usetikzlibrary{arrows}       %                 ...customizing arrows
\newcommand{\eqdef}{=\vcentcolon}
\usepackage[top=3cm,left=3cm,right=3cm,bottom=3cm]{geometry}
\newcommand{\mref}[3][red]{\hypersetup{linkcolor=#1}\cref{#2}{#3}\hypersetup{linkcolor=blue}}%<<<changed

\tikzset{node distance=4.5cm, % Minimum distance between two nodes. Change if necessary.
         every state/.style={ % Sets the properties for each state
           semithick,
           fill=cyan!40},
         initial text={},     % No label on start arrow
         double distance=4pt, % Adjust appearance of accept states
         every edge/.style={  % Sets the properties for each transition
         draw,
           ->,>=stealth',     % Makes edges directed with bold arrowheads
           auto,
           semithick}}


% Start of document.
\newcommand{\sep}{\hspace*{.5em}}

\begin{document}
\title{Natural Language Processing}
\author{James Zhang\thanks{Email: \mailto{jzhang72@terpmail.umd.edu}}}
\date{\today}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\maketitle
\newpage
\tableofcontents
\newpage

\section{Word Vectors}

How do we represent the meaning of a word?

From Merriam Webster, a "meaning" is 
\begin{enumerate}
    \item The idea that is represented by a word, phrase, etc
    \item The idea that a person wants to express by using word, signs, etc
    \item The idea that is expressed in a work of writing, art, etc
\end{enumerate}

\begin{definition}
    A $\vocab{word vector}$ is the meaning of a word stored in a dense vector. We build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.
\end{definition}

\begin{note}
    Word vectors are synonomous with $\vocab{word embeddings}$ or 
    
    $\vocab{neural word representations}$.
\end{note}

\begin{example}
    Suppose we have the word "king" represented as a vector $v \in \RR^{300}$, in other words this high-dimensional vector has 300 dimensions. In high dimensional space, words that are used in similar contexts are close together. Ideally, if we have the words, "king", "man", "woman", "queen" - all represented as vectors, then we hope to achieve something like 
    \[\text{king - man + woman = queen}\]
\end{example}

\begin{definition}
The process of representing a word by a vector is represented by a class of methods known as $\vocab{word embedding algorithms}$.
\end{definition}

\begin{note}
    Typically, word embeddings are trained by optimizing an auxiliary function in a large, unsupervised learning dataset ie. predicitng a word based on its context. These embeddings have proven to be efficient and fast in computing core NLP tasks.
\end{note}

\begin{note}
    Measuring similarity between vectors is possoble using measures such as cosine similarity.
\end{note}

Word embeddings are often used as the first data processing layer in a deep learning model, and we will build on this more when we get to BERT.

\subsection{WordNet}

WordNet was one of the earliest ways people tried to quantitatively store a word's meaning but it failed because it
\begin{itemize}
    \item was missing nuance.
    \item was missing new meanings of words.
    \item was impossible to keep up to date.
    \item used distributional semantics ie. a word's meaning is given by the words that frequently appear close to it.
\end{itemize}

\subsection{Word2Vec}

\begin{definition}
    The $\vocab{Word2Vec}$ algorithm showed that we can use a vector to properly represent words and syntactic, grammar-based relationships.
\end{definition}

Suppose we have a corpus of text. We must learn a good vector for each word, and a good word vector is learned from Stochastic Batch Gradient Descent, where the batch is small and we need to minimize our cost function.

Take a center word $c$, and the set of words around it $o$. We wish to be able to predict context words in a window and give high probability to words that occur within a context, essentially how likely a word is related to another word ie. optimize this loss function.
    
\begin{example}
    We can potentially represent relationships in a co-occurence matrix. Imagine a covariance matrix but instead of covariances it's the number of sentences in the corpus in which the $i$ word and the $j$ word both appear. Note that this matrix is symmetrical.
\end{example}

Note that once we obtain all occurrences, we can then begin to model them as probabilities.

\begin{example}
    If "basketball" and "hoop" appear 5 times together in a corpus, and "basketball" and "ice" appear one time together in a corpus, then 
    \[\mathbb{P}(x = \text{ice} \ | \ \text{basketball}) \text{ is small but } \mathbb{P}(x = \text{hoop} \ | \ \text{basketball}) \text{ is large.}\]
    This embeds meaning within co-occurrence. 
\end{example}

We will also have a softmax function at the end that maps values $x$ to a probability distribution $p$, where probability is calculated by the dot product from the center $c$ to the context word vector. We then convert softmax score to probabilities.

To actually make a word vector, for each word, we will have 2 vectors that are $d-$dimensional for both center and surroundings words. These 2 vectors will change as you move through a sentence, sort of representing a window. Place words that are similar using the co-occurrence matrix process close together in a high-dimensional space.

\subsubsection{Implementations}

Word2Vec can be implemented with 2 architectures.
\begin{enumerate}
    \item Skip-grams predicts the surrounding context words given the central target word.
    \item CBOW computes the conditional probability of a target word given the context words surrounding it across a window of size $k$.
\end{enumerate}

\subsection{Limitations of Word Embeddings}
    

\section{NLP Tasks}

\section{Neural Networks}

\section{Language Models}

\section{Machine Translation}

\section{Attention}

\section{Knowledge Graphs}

\section{Transformer}

\section{Tokenizer}

\end{document}
