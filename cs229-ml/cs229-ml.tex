\documentclass[12pt]{scrartcl}
\usepackage[sexy]{ekesh}
\usepackage[noend]{algpseudocode}
\usepackage{answers}
\usepackage{array}
\usepackage{tikz}
\newenvironment{allintypewriter}{\ttfamily}{\par}
\usepackage{listings}
\usepackage{xcolor}
\usetikzlibrary{arrows.meta}
\usepackage{color}
\usepackage{mathtools}
\newcommand{\U}{\mathcal{U}}
\newcommand{\E}{\mathbb{E}}
\usetikzlibrary{arrows}
\Newassociation{hint}{hintitem}{all-hints}
\renewcommand{\solutionextension}{out}
\renewenvironment{hintitem}[1]{\item[\bfseries #1.]}{}
\renewcommand{\O}{\mathcal{O}}
\declaretheorem[style=thmbluebox,name={Chinese Remainder Theorem}]{CRT}
\renewcommand{\theCRT}{\Alph{CRT}}
\setlength\parindent{0pt}
\usepackage{sansmath}
\usepackage{pgfplots}

\usetikzlibrary{automata}
\usetikzlibrary{positioning}  %                 ...positioning nodes
\usetikzlibrary{arrows}       %                 ...customizing arrows
\newcommand{\eqdef}{=\vcentcolon}
\usepackage[top=3cm,left=3cm,right=3cm,bottom=3cm]{geometry}
\newcommand{\mref}[3][red]{\hypersetup{linkcolor=#1}\cref{#2}{#3}\hypersetup{linkcolor=blue}}%<<<changed

\tikzset{node distance=4.5cm, % Minimum distance between two nodes. Change if necessary.
         every state/.style={ % Sets the properties for each state
           semithick,
           fill=cyan!40},
         initial text={},     % No label on start arrow
         double distance=4pt, % Adjust appearance of accept states
         every edge/.style={  % Sets the properties for each transition
         draw,
           ->,>=stealth',     % Makes edges directed with bold arrowheads
           auto,
           semithick}}


% Start of document.
\newcommand{\sep}{\hspace*{.5em}}

\begin{document}
\title{Stanford CS229: Machine Learning}
\author{James Zhang\thanks{Email: \mailto{jzhang72@terpmail.umd.com}}}
\date{\today}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\maketitle
\newpage
\tableofcontents
\newpage

\section{Linear Regression}

\subsection{Notation}
\begin{itemize}
    \item $x^{(i)}$ denotes input variables of features, and $y^{(i)}$ denotes the output or target variable
    \item $x^{(i)} \in \RR^{n+1}$ because we have a fake feature $x_0 = 1$
    \item $y^{(i)} \in \RR$ always in the case of regression
    \item $x_j^{(i)}$ represents the j-th feature of the i-th training example
    \item $m$ denotes the number of training examples
    \item $n$ denotes the number of features
    \item $h(x)$ denotes the hypothesis function, which is a linear function of the features $x$ and $x_0 = 1$
    \item $J(\vec{w})$ denotes the cost function you're trying to minimize by finding the optimal set of parameters $\vec{w}$ to straight line fit the data
    \item $X$ denotes the space of input values, and $Y$ denotes the space of output values
\end{itemize}

\subsection{Hypothesis Function}
\begin{definition}
We represent our $\vocab{hypothesis function}$ as a linear function of $x$, such that our values $w_i$ are weights
\[h(x) = \sum_{i=0}^nw_ix_i = w \cdot x = w^Tx\]
such that $w$  and $x$  are both vectors, and $n$ is the number of input variables.
    
\end{definition}

\subsection{Cost Function}
\begin{definition}
    Now let us define the $\vocab{least squares cost function}$ that gives rise to the ordinary least squares regression model.
\[J(\vec{w}) = \frac{1}{2m} \sum_{i=1}^m(h(x^{(i)} - y^{(i)})^2\]
and we want to choose $\vec{w}$ so as to minimize the cost function, and this is known as the $\vocab{gradient descent algorithm}$.
\[w_j := w_j - \alpha \frac{\partial}{\partial w_j}J(w)\]
and this is performed for all values of $j \in [0, n]$. Note that $\alpha$ is the learning rate.
\end{definition}
\subsection{LMS (Widrow-Hoff) Learning Rule}
Let us solve for $\frac{\partial}{\partial w_j}J(\vec{w})$ such that we can substitute it into our formula for gradient descent.
\[\frac{\partial}{\partial w_j}J(\vec{w}) = \frac{\partial}{\partial w_j}\frac{1}{2}(h(x) - y)^2\]
\[= (h(x) - y) \cdot \frac{\partial}{\partial w_j}(h(x) - y)\]
\[= (h(x) - y) \cdot \frac{\partial}{\partial w_j}(\sum_{i=0}^nw_ix_i - y)\]
\[= (h(x) - y)x_j\]
\begin{definition}
    For a single training example, the update rule is 
\[w_j := w_j + \alpha(y^{(i)} - h(x^{(i)}))x_j^{(i)}\]
and this is known as the LMS (Least Mean Squares) and the Widrow-Hoff Learning Rule and $\vocab{stochastic gradient descent}$.
\end{definition}

\subsection{Stochastic vs. Batch Gradient Descent}
\begin{note}
    Parameters may keep oscillating around the local extreme, but it will get there way faster than batch gradient descent.
\end{note}
\begin{note}
    We can ensure that this algorithm converges if as we get close to the local extreme, let $\alpha \to 0$.
\end{note}
\begin{definition}
    To modify this for more than one example and this is \\ $\vocab{batch gradient descent}$.
\[w_j:= w_j + \alpha \sum_{i=1}^m(y^{(i)} - h(x^{(i)}))x_j^{(i)}\]
\end{definition}
\begin{note}
    When the training set $m$ is large, stochastic gradient descent is preferred over batch gradient descent.
\end{note}
\subsection{The Normal Equations}
Instead of using the iterative algorithm of gradient descent, we can minimize our cost function $J(\vec{w})$ by explicitly taking derivatives with respect to the $w_j$ and set them to 0.
\subsubsection{Matrix Derivatives}
Suppose we have a function $f: \RR^{m \times n} \to \RR$ that maps from an $m \times n$ matrix to the real numbers, the derivative of $f$ with respect to matrix $A$ is 
$$\nabla_A f(A) = \begin{bmatrix}
    \frac{\partial f}{\partial A_{11}} & \cdots & \frac{\partial f}{\partial A_{1n}}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial f}{\partial A_{m1}} & \cdots & \frac{\partial f}{\partial A_{mn}}
\end{bmatrix}$$
\begin{example}
    Suppose $$A = \begin{bmatrix}
        A_{11} & A_{12}\\
        A_{21} & A_{22}
    \end{bmatrix}$$ is a $2 \times 2$ matrix, and the function $f: \RR ^{2 \times 2} \to \RR$ is given by 
    \[f(A) = \frac{3}{2}A_{11} + 5A_{12}^2 + A_{21}A_{22}\]
    then 
$$\nabla_A f(A) = \begin{bmatrix}
    \frac{3}{2} & 10A_{12}\\
    A_{22} & A_{21}
\end{bmatrix}$$
\end{example}

Now let us introduce the $\vocab{trace}$ operator such that the trace of a matrix $A$ is the sum of its diagonal entries.
\[\Tr A = \sum_{i=1}^n A_{ii}\]
\begin{lemma}
    \[\Tr AB = \Tr BA\]
    \[\Tr ABC = \Tr CAB = \Tr BCA\]
    \[\Tr A = \Tr A^T\]
    \[\Tr(A + B) = \Tr A + \Tr B\]
    \[\Tr a A = a \Tr A\]
\end{lemma}
Note that the fourth equation only applies to non-singular square matrices.
\begin{lemma}
\[\nabla_A \Tr AB = B^T\]
\[\nabla_{A^T}f(A) = (\nabla_A f(A))^T\]
\[nabla_A \Tr (ABA)^TC = CAB + C^TAB^T\]
\[\nabla_A \det(A) = \det(A)(A^{-1})^T\]
\end{lemma}

\subsubsection{Least Squares Revisited}

We seek a closed form of $\vec{w}$ that minimizes $J(\vec{w})$. Let us rewrite $J(\vec{w})$ in matrix-vectorial notation.

\begin{proof}
    Let $X$ be an $m \times n$ matrix such that each column represents a training example.
    \[X = \begin{bmatrix}
        (x^{(1)})^T & \cdots & (x^{(m)})^T
    \end{bmatrix}\] and let \[\vec{y} = \begin{bmatrix}
        y^{(i)} \\
        \vdots \\
        y^{(m)}
    \end{bmatrix}\]
    be an $m \times 1$ vector.
Recall that $h(x^{(i)}) = (x^{(i)})^Tw$ and therefore
\[X\vec{w} - \vec{y} = \begin{bmatrix}
    h(x^{(1)}) - y^{(1)}\\
    \vdots\\
    h(x^{(m)}) - y^{(m)}\\
\end{bmatrix}\]
Therefore,
\[\frac{1}{2}(X\vec{w}-\vec{y})^T(X\vec{w} - \vec{y}) - \frac{1}{2}\sum_{i=1}^m (h(x^{(i)} = y^{(i)})^2 = J(\vec{w})\]
Thus we've represented our cost function in terms of matrices, but now let us minimize it using the trace equations above.
\[\nabla_{\vec{w}J(\vec{w}} = \nabla_{\vec{w}}(X\vec{w} - \vec{w})^T(X\vec{w} - \vec{y})\]
\[ = \frac{1}{2}\nabla_{\vec{w}} (\vec{w}^TX^T X\vec{w} - \vec{w}^T X^T \vec{y} -\vec{y}^TX\vec{w} + \vec{y}^T\vec{y})\]
by homogeneity, transpose of a matrix, and distributive property.
\[= \frac{1}{2}\nabla_{\vec{w}} \Tr (\vec{w}^TX^T X\vec{w} - \vec{w}^T X^T \vec{y} -\vec{y}^TX\vec{w} + \vec{y}^T\vec{y})\] because the trace of a real number is just the real number
\[= \frac{1}{2}\nabla_{\vec{w}}(\Tr \vec{w}^TX^TX\vec{w} - 2\Tr\vec{y}^TX\vec{w})\]
using the property $\Tr A = \Tr A^T$.
\[= \frac{1}{2}(X^TX\vec{w} + X^TX\vec{w} - 2X^T\vec{y})\]
\[= X^TX - X^T\vec{y} \implies X^TX\vec{w} = X^T\vec{y} \implies \vec{w} = (X^TX)^{-1}X^T\vec{y}\]
\end{proof}

\subsection{Probabilistic Interpretation: Why Squared Error}

Suppose in our linear regression model
\[y^{(i)} = \vec{w}^Tx^{(i)} + \epsilon^{(i)}\]
where $\epsilon^{(i)}$ is an error term. Let's now assume that $\epsilon^{(i)}$ follows a Gaussian distribution of mean $\mu = 0$ and variance $\sigma^2$, which is reasonable.
\[\epsilon^{(i)} \sim N(0,\sigma^2) \implies \mathbb{P}(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(e^{(i)})^2}{2\sigma^2})\]
Now let us assume that each $\epsilon^{(i)}$ is $\vocab{IID (independently and identically distributed)}$ \\from each other. This may not be entirely true, but it's good enough to get a good approximation for a model. Therefore, we can express the following
\[\mathbb{P}(y^{(i)} \ | \ x^{(i)}; \vec{w}) = \frac{1}{\sqrt{2\pi}\sigma }\exp(-\frac{(y^{(i)} - \vec{w}x^{(i)})^2}{2\sigma^2})\]
by substitution.
Essentially, given $\vec{x}, \vec{w}$, what's the probability density of a particular house's price?
\[y^{(i)} \ | \ x^{(i)}; \vec{w} \sim N(\vec{w}^Tx^{(i)}, \sigma^2)\]
The random variable $y^{(i)}$ given $x^{(i)}$ parameterized by $\vec{w}$ is that Gaussian. Weâ€™re essentially modeling the price of a house using the random variable y.
\begin{definition}
Below, we have the definition of a $\vocab{likelihood function}$ in terms of $\vec{w}$.
\[L(\vec{w}) = L(\vec{w}; X, \vec{y}) = \mathbb{P}(\vec{y} | X; \vec{w})\]    
\end{definition}
Because we assumed that all $\epsilon^{(i)}$ are IID, the probability of all of the observations in the training set is simply the product
\[L(\vec{w}) = \mathbb{P}(\vec{y} | X; \vec{w}) = \prod_{i=1}^m \mathbb{P}(\vec{y} | X; \vec{w} = \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma }\exp(-\frac{(y^{(i)} - \vec{w}x^{(i)})^2}{2\sigma^2})\]
Our best guess of parameters $\vec{w}$ is by choosing $\vec{w}$ such that we maximize $L(\theta)$ which intuitively makes sense because we want to maximize the probability that given our $x$ value, and parameterized by the weight, our probability that our ouputted value of $y$ is as high as possible.
\begin{definition}
    Instead of maximizing $L(\vec{w})$, we will maximize \\the $\vocab{log likelihood}$.
\[l(\vec{w}) = \log L(\vec{w}) = \log \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma }\exp(-\frac{(y^{(i)} - \vec{w}x^{(i)})^2}{2\sigma^2})\]
\end{definition}
\begin{note}
    The product rule of logarithms
    \[\log xy = \log x + \log y\]
\end{note}
\[l(\vec{w}) = \sum_{i=1}^m \log \frac{1}{\sqrt{2\pi}\sigma }\exp(-\frac{(y^{(i)} - \vec{w}x^{(i)})^2}{2\sigma^2})\]
\[l(\vec{w}) = \sum_{i=1}^m \left[\log \frac{1}{\sqrt{2\pi}\sigma } + \log \exp(-\frac{(y^{(i)} - \vec{w}x^{(i)})^2}{2\sigma^2})\right]\]
\[l(\vec{w}) = m \log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2}\sum_{i=1}^m (y^{(i)} - \vec{w}^Tx^{(i)})^2\]
\begin{note}
    Note that the first term is constant, so we wish to only consider the second term. Now, observe that in the second term, we can ignore the $\frac{1}{\sigma^2}$ because it is also constant. Finally, since the second term is negative, note that maximizing the negative of a term is the same as minimizing the term. Hence, maximizing $\l(\vec{w})$ is equivalent to minimizing
    \[\frac{1}{2}\sum_{i=1}^m(y^{(i)} - \vec{w}^Tx^{(i)})^2\]
    which is $J(\vec{w})$, our original least squares cost function.
\end{note}

\section{Locally Weighted Linear Regression}

\subsection{Fitting Your Data}

You can either fit 
\begin{itemize}
    \item Linear of the form $w_0 + w_1x_1$
    \item Quadratic of the form $w_0 + w_1x_1 + w_2x_2^2$
    \item Some custom fit?
\end{itemize}
Whatever way, the linear regression naturally fits these functions of input features in your dataset.

\subsection{Locally Weighted Linear Regression}
\begin{definition}
    $\vocab{Locally Weighted Linear Regression}$ is an algorithm that modifies linear regression to fit non-linear functions.
\end{definition}

\begin{note}
    The more features we add does not always correspond to better performance because it could lead to overfitting.
\end{note}

Recall that the original cost function was
\[J(\vec{w}) = \frac{1}{2}\sum_{i = 1}^m (y^{(i)} - \vec{w}^T x^{(i)})^2\] To make a prediction at an input $x$ using locally weighted linear regression
\begin{itemize}
    \item Fit a straight line through the local neighborhood of training examples close to $x$.
    \item Fit a line passing through the outputs $y$ in that neighborhood, and make a prediction at $x$.
    \item Note that by focusing on the local neighborhood of input points, we attribute these points to influence the output prediction the most, but the other points can also be weighted lesser and utilized for our prediction.
\end{itemize}
\begin{definition}
    We can represent this in a $\vocab{weighted cost function}$ such that 
\[J(\vec{w}) = \sum_{i=1}^m w'^{(i)}(y^{(i)} - \vec{w}^Tx^{(i)})^2\]
where $w'$ is a weighting function with values $\in [0, 1]$. A common choice for $w'$ is 
\[w'^{(i)} = \exp(-\frac{x^{(i)} - x)^2}{2\tau^2})\]
\end{definition}
\begin{note}
    The closer the neighbor ie. $|x^{(i)} - x| \to 0$ the more heavily weighted as in $w' \approx e^0 = 1$. The counter logic applies as well. The further a neighbor, the smaller the weight.
\end{note}
\begin{note}
    The weight function is not a normal distribution, and the weights are not random variables.
\end{note}

\subsection{Bandwidth Parameter}

\begin{definition}
    The $\vocab{bandwidth parameter}$ $\tau$ decides the width of the neighborhood.
\end{definition}

\begin{itemize}
    \item $\tau$ controls how quickly the weight of a training example $x^{(i)}$ falls off as distance from the query point $x$ increases, and $\tau$ can have a big impact on potentially underfitting and overfitting
    \item $\tau$ is a hyperparameter of this algorithm, and it doesn't have to be bell-shaped.
\end{itemize}

\subsection{Applications}

The main benefit of this algorithm is that fitting a non-linear dataset doesn't require you to manually fiddle with features.

\begin{note}
    This works best when the number of features isn't too large, say 2 or 3.
\end{note}

In terms of computation and space complexity, you need to solve a linear system of equations of dimension equal to $m$ so we may need to resort to scaling algorithms.

\section{Newton's Method}

\begin{definition}
Gradient ascent is very effective, but it needs many iterations before
convergence. $\vocab{Newton's method}$ takes bigger steps
and converges earlier when maximizing the likelihood function
$l(\vec{w})$. Although there are less iterations, each
iteration tends to be more expensive.
\end{definition}

\begin{lemma}
    Suppose we have a function $f: \RR \to \RR$
    and a scalar value $w$. The update rule 
    to find the zero of a function is the following
    \[w := w - \frac{f(w)}{f'(w)}\]

    The nautral interpretation of this formula is 
    we take our function $f$ and approximate it with a linear
    function tangent to $f$ at the current value of 
    $w$, and solving for where this linear function 
    equals 0 (the x-intercept) gives us our next guess
    for $w$.
\end{lemma}

Now suppose we wish to maximize the log likelihood 
function $l(w)$. The maxima of $l(w)$ corresponds to points
where its first derivative is equal to 0. Thus, let 
\[f(w) = l'(w) \implies w := w - \frac{l'(w)}{l''(w)}\]
In terms of indices, 
\[w^{(t+1)} = w^{(t)} - \frac{l'(w)}{l''(w)}\]

\subsection{Quadratic Convergence}
Note that when you apply Newton's method,
the iterations represent quadratic steps 
such that the number of significant digits that
you have converged to the minimum double on a single 
iteration.

\subsection{Newton's Method for Vectors}
\begin{definition}
    If $w$ is a vector and not a scalar, we have
to generalize Newton's method to a multi-dimensional setting,
and this is known as the 
\\ \vocab{Newton-Raphson method}
\[\vec{w} := \vec{w} - H^{-1}\nabla_0l(\vec{w})\]
\end{definition}
where
\begin{itemize}
    \item $\nabla_0l(\vec{w})$ is the vector of partial derivatives
with respect to the $w_i's$
\item H is the Hessian square matrix defined as 
\[H_{ij} = \frac{\partial^2 l(\vec{w})}{\partial w_i \partial w_j}\]
\end{itemize}

\section{The Classification Problem}

\begin{definition}
Let us take this framework that we have developed and 
apply it to a \vocab{classification} problem. This differs from 
regression because the $y$ values can only take on a discrete
number of values. Let us simplify this even further by beginning with the 
binary classification problem such that $y \in 0, 1$
where $0$ is the negative class and $1$ is the 
positive class.  
\end{definition}

\begin{note}
    The problem with applying linear regression to a 
    classification problem and applying a threshold of 0.5 
    is that if there is a single outlier, the regression
    algorithm will adjust its straight line, which could drastically
    alter the decision boundary. Furthermore, the decision
    boundary for classification problems may not be clear cut.
\end{note}

\subsection{Logistic Regression}


\begin{definition}
    If we know output values will be between 0 and 1, 
it makes sense to choose our hypothesis function 
$h(x)$ as the following
\[h(x) = g(w^Tx) = \frac{1}{1 + e^{-w^Tx}}\]
where $g(z) = \frac{1}{1 + e^{-z}}$ is known as 
the \vocab{logistic function} or the sigmoid function.
\end{definition}

\begin{lemma}
    When we will later try to adjust $w$ such as to 
    minimize our cost function, we must be able to compute the 
    derivative of the cost function, and thereby we must be able 
    to compute the derivative of the sigmoid function.
    \[g'(z) = \frac{d}{dz}\frac{1}{1 + e^{-z}}
    = g(z)(1-g(z))\]
    A rigorous derivation can be found online.
\end{lemma}

\subsection{Update Rule}

Let's use a similar technique of making some 
probabilistic assumptions and then fitting the parameters via a
a maximum likelihood. Note that because $y$ only takes
2 values, 0, and 1
\[\mathbb(y^{(i)} = 1 | x^{(i); w}) = h(x^{(i)})\]
\[\mathbb(y^{(i)} = 0 | x^{(i)}; w) = 1 - h(x^{(i)})\]
Assuming that the $m$ training examples were 
generated independently, we have the likelihood function
as follows
\[L(w) = \mathbb{P}(\vec{y} | X; w) = \prod_{i=1}^m\mathbb{P}(y^{(i)} | x^{(i)}; w)
= \prod_{i=1}^m h(x^{(i)})^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}\]
Note that it's easier instead to maximumze the log likelihood 
rather than the actual likelihood, as local extreme will occur in the same 
points in both.
\[l(w) = \log L(w) = \sum_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log(1-h(x^{(i)}))\]
Now we will use batch gradient ascent to obtain the update rule
\[w:= w + \alpha \nabla_wl(w)\]
\begin{note}
    Note that there's a plus sign because we are using
    gradient ascent, not descent, and here we are maximizing the 
    log-likelihood function instead of minimizng the 
    squared cost function like in linear regression.
\end{note}
Now if we plug in our sigmoid function value of $h(x)$, and then solve for 
$\frac{\partial l(w)}{\partial w_j}$ we get the following 
\[\frac{\partial l(w)}{\partial w_j} = (y-h(x^{(i)}))x_j^{(i)}\]
which yields the update rule for an entire training set $m$ of examples
\[w_j = w_j + \alpha \sum_{i=1}^m (y^{(i)} - h(x^{(i)})x_j^{(i)})\]
While this appears to be the same as the LMS update rule, it is not because 
now our hypothesis function is now not a linear function of $w^Tx$. However, this is not a
coincidence, and this update rule is a  property of a bigger class of algorithms called
\vocab{generalized linear models}.

\section{Generalized Linear Models}
\begin{itemize}
    \item In linear regression, $y | x; \vec{w} \sim N(\mu, \sigma^2)$
    \item In logistic regression $y | x; \vec{w} \sim \text{Bernoulli}(p)$
\end{itemize}

Both of these methods are part of a broader family of models, known as Generalized Linear Models.

\subsection{The Exponential Family}
\begin{definition}
    Let us first define \vocab{exponential family} distributions. A class of distributions is in the exponential family if their PDF can be written in the form.
\[P(y; \eta ) = b(y)\exp(\eta^TT(y) - a(\eta))\]
where
\begin{itemize}
    \item $y$ is the data that the PDF is trying to model
    \item $\eta$ is called the \vocab{natural (canonical) parameter} of the distribution
    \item $T(y)$ is called the \vocab{sufficient statistic} (often $T(y) = y$)
    \item $b(y)$ is the \vocab{base measure}
    \item $a(\eta)$ is called the \vocab{log partition function}
\end{itemize}
\end{definition}

Rewriting the form above as 
\[\mathbb{P}(y; \eta) = b(y) \frac{\exp(\eta^TT(y))}{\exp(a(\eta))}\] such that the denominator plays the role of a normalization constant to ensure that the PDF integrates over $y$ to 1.

\begin{note}
Note that 
    \begin{itemize}
        \item $y$ is a scalar
        \item $\eta$ is only a vector when considering softmax regression (multiclass classification)
        \item $T(y)$ is a vector
        \item $b(y)$ and $a(\eta)$ are scalars
    \end{itemize}
\end{note}
Now let us show that Bernoulli Distribution and Normal Distributions are examples of exponential family distributions.

\subsubsection{Bernoulli Distribution}

The Bernoulli Distribution is a distribution used to model binary data.
\[P(y = 1) = p, P(y = 0) = 1 - p\]
\[E(X) = p, \text{Var}(X) = p(1-p)\]
\[P(y) = p^y(1-p)^{1-p}\]
is the pdf. We seek to take th pdf and convert it to an equation of the form that defines the distribution to be a member in the exponential family. Recall that that form is 
\[P(y; \eta) = b(y)\exp(\eta^TT(y) - a(\eta))\]
\begin{proof}
\[P(y) = p^y(1-p)^{1-p}\]
\[P(y) = \exp(y\log p + (1-y)\log(1-p))\]
\[P(y) = \exp((\log(\frac{p}{1-p}))y + \log(1-p))\]
\begin{itemize}
    \item Base measure $b(y)$ is given by 1
    \item Sufficient statistic $T(y)$ is given by $y$
    \item Log partition function $a(\eta)$ is given by $\log (1-p)$
    \item Natural parameter $\eta$ is given by $-\log \frac{p}{1-p}$
\end{itemize}
Note that if we solve the last equation for $p$ instead of $\eta$, we obtain the sigmoid function such that 
\[p = \frac{1}{1 + e^{-\eta}}\]
Note that our log partition function is currently not in terms of $\eta$ but we can get it to be so by plugging in our sigmoid function for $p$. Thus, 
\[a(\eta) = -\log(1-p) = -\log(1-\frac{1}{1 + e^{-\eta}}) = \log(1+ e^\eta)\]
Therefore, the Bernoulli Distribution is a member of the exponential family.
\end{proof}

\subsubsection{Gaussian Distribution}

Proving that the Gaussian Distribution is a member of the exponential family is a similar process. Write out the pdf, and simply use algebra to massage it into our desired form. In this case, note that $\eta \in \RR^2$.

\subsubsection{Other Members of the Exponential Family}

\begin{itemize}
    \item The mulitnomial distribution, which we'll use in your discussion on softmax regression
    \item Regression $\to$ you could model the $y's$ as a Gaussian
    \item Binary classification $\to$ Bernoulli Distribution
    \item Poisson distribution for modelling count data which consits of non-negative integers (number of people who will enter your store in an hour)
    \item Gamma and exponential distributions for modeling continuous values such as time intervals or volume of an object
    \item Beta and Dirichlet distributions show up in Bayesian ML or Bayesian statistics
\end{itemize}

\subsubsection{Properties of the Exponential Family}
\begin{itemize}
    \item Performing maximum likelihood estimation on a distribution parameterized by the natural parameter yields a concave function.
    \item Minimizing the negative log likelihood yields a convex function. Note that NLL is essentially the cost function equivalent of maximum likelihood estimation.
\end{itemize}
\begin{note}
    Note that 
    \[E(y; \eta) = \frac{\partial}{\partial \eta}a(\eta) \text{ and Var}(y; \eta) = \frac{\partial^2}{\partial\eta^2}a(\eta)\]
    These last two properties are especially noteworthy because normally when finding mean and variance, they require integration, but here we can find both with just differentation.
\end{note}

\subsection{Generalized Linear Models (GLMs)}

GLMs are a natural extension of the exponential family. They build powerful models using a distribution that belongs to the exponential family and plugging it in at the output of a linear model, which models the output $y$ as a linear function of the input features $x$.
\begin{itemize}
    \item Depending on the task at hand, choose parameters $b, a, T(\cdot)$ based on the distribution that you'd like to model your output as.
    \item During training time, note that we're learning and tuning the parameters of $\vec{w}$. The output of thias model will become $\eta, $ the natural parameter to our distribution.
    \item During test time, the mean of our distribution which is the first derivative of the log partition function with respect to $\eta$ will become our prediction $y$.
\end{itemize}

\subsubsection{Update Rule}

No matter the choice of the exponential family distribution that you decide to model your output $y$ values as, the update rule remains the same
\[w_j := w_j + \alpha (y^{(i)} - h(x^{(i)})x_j^{(i)}\]

Note that this for stochastic gradient descent. For batch, just sum over all of the examples before updating.
Simply initialize random weights and plug them into an appropriate hypothesis function and start learning right away.

\begin{note}
    Newton's method is the most common MLE algorithm you would use with GLMs, assuming the number of futures is fewer than a thousand.
\end{note}

Observe that most of time, our sufficient statistic $T(y) = y$ and this implies that 
\[g(\eta) = E(T(y); \eta) = E(y; \eta) = \mu = \frac{\partial}{\partial \eta}a(\eta)\]
where this function pair $g, g^{-1}$ is known as the canonical link function, and $g$ is a function that yields the distribution's mean as a function of the natural parameter.

\subsubsection{Parameterization Spaces}

Note that now we have three parameterization spaces
\begin{enumerate}
    \item $\vec{w} \to $ during the training phase of a GLM, we learn $w$.
    \item $\eta \to $ where $\eta = \vec{w}^T\vec{x}$
    \item Canonical parameters such as $p, (\mu, \sigma^2), \lambda$
\end{enumerate}

\subsubsection{Constructing GLMs}

Now we will actually discuss constructing GLMs to solve problems. Before we do, however, we must identify three assumptions about our conditional distribution $y$ given $x$.
\begin{itemize}
    \item $y | x; \vec{w} \sim \text{Exponential Family}$
    \item $\eta = \vec{w}^Tx$ and if $\eta$ is vector valued then $\eta_i = \vec{w}_i^Tx$ where $\vec{w} \in \RR^n$ and $x \in \RR^n$. This isn't necessarily a needed assumption, but more of a design pattern.
    \item Our goal is to output a prediction given by the mean of our distribution, in other words, $h(x) = E(y | x; \vec{w})$
\end{itemize}

\subsection{Linear Regresion (Gaussian Distribution)}

Our goal use our assumptions and properties to begin from the fact that we wish to output a prediction given by the mean of our distribution. Working backwards will allow us to verify our hypothesis function. For example, in linear regression
\[h(x) = E(y | x; \vec{w}) = \mu = \eta = \vec{w}^Tx\]

\subsection{Logistic Regression (Bernoulli Distribution)}
\[h(x) = E(y | x; \vec{w}) = p = \frac{1}{1 + e^{-\eta}} = \frac{1}{1 + e^{-\vec{w}^T}}\]

\subsection{Softmax Regression (Multinomial Distribution)}

\begin{definition}
Note that \vocab{softmax regression} is to \vocab{multiclass regression} in the same way that logistic regression is to binary classification. Let's derive a GLM for this problem, which we will start by expressing a multinomial as an exponential family distribution.
    
\end{definition}

Let us denote $k$ parameters $\phi_i, \cdots, \phi_k$. Note that $\sum_{i=1}^k \phi_i = 1$, and as such we will only parameterize $k-1$ parameters such that $\phi_k = 1 - \sum_{i=1}^{k-1}\phi_i$, but $\phi_k$ is not a parameter.

Now let us denote $T(y) \in \RR^{k-1}$ such that they form a standard basis set
\[T(1) = e_1, T(2) = e_2, \cdots ,T(k-1) = e_{k-1}, T(k) = \text{zero vector}\]

Now let us also define an indictor function $1\{\cdot\}$ which returns 1 if its argument is true and 0 otherwise.

\[P(y; \phi) = \phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\cdots\phi_k^{1\{y=k\}}\]
\[= \phi_1^{T(1)}\cdots\phi_k^{1-\sum_{i=1}^{k=1}T(i)}\]
\[= \exp(T(1)\log(\frac{\phi_1}{\phi_k} + \cdots + T(k-1)\log(\frac{\phi_{k-1}}{\phi_k} + \log(\phi_k)\]
\[= b(y)\exp(\eta^TT(y) - a(\eta))\]
where 
\[n = \begin{pmatrix}
    \log(\phi_1/\phi_k)\\
    \vdots\\
    \log(\phi_{k-1}/\phi_k)
\end{pmatrix}, a(\eta) = -\log(\phi_k), b(y) = 1\]
A little bit more algebraic manipulation gets you the formula
\[P(y=i| x;\vec{w}) = \frac{e^{\vec{w}_i^Tx}}{\sum_{j=1}^k e^{\vec{w}_j^Tx}}\]

\section{Generative Learning Algorithms}

We have mainly observed learning algorithms that model the conditional distribution $P(y | x; w)$. For instance, with logistic regression, we have $h(x) = g(w^Tx)$ where $g$ is the sigmoid function. In this section, we'll focus on binary classification to talk about a different learning algorithm.\\

Consider a classification problem where we want to distinguish between malignant $y=1$ and benign $y=0$ tumors.
\begin{itemize}
    \item All learning algorithms such as logistic regression, perceptron algorithm, or generalized learning model starts with randomly initialized parameters. Over the course of learning, the algorithm performs gradient descent where the decision boundary evolves until you obtain a boundary that separates the two classes.
    \item To classify a new sample, it checks on which side the of the decision boundary the new sample falls in, and makes an appropriate prediction.
\end{itemize}
\begin{definition}
    These algorithms that try to learn $P(y | x)$ directly or map from the input space to the labels are known as \vocab{discriminative learning algorithms}.
\end{definition}
The alternative method is to 
\begin{itemize}
    \item First, look at the malignant tumors and see what the features look like. Then look at benign tumors, and we can build a separate model of what benign tumors look like.
    \item To classify a new tumor, match the new sample against the malignant tumor model and the benign tumor model, and see whether the new sample is more similar to which model. 
\end{itemize}
\begin{definition}
    In this section, we'll talk about algorithms that model $P(x | y)$ and $P(y)$. These are called \vocab{generative learning algorithms}.
\end{definition}
\begin{note}
    In this algorithm, we observe $x$ given the label $y$. Therefore, $P(x | y=0)$ models the distribution of benign tumors' features, and $P(x | y = 1)$ models the distribution of malignant tumors' features.
\end{note}
\begin{definition}
    The model also learns the \vocab{class prior}, or the independent probability $P(y)$. For example, when a patient walks into a hospital, the doctor already knows the probability that a patient will have a malignant vs benign tumor.
\end{definition}

After modeling $P(y)$ (the class priors) and $P(x | y)$, the algorithm can use \vocab{Bayes' rule}
to derive $P(y | x)$
\[P(y | x) = \frac{P(x | y)P(y)}{P(x)}\]
\begin{note}
    Note that you don't actually need to calculate the denominator $P(x) = P(x | y = 1)P(y = 1) + P(x | y = 0)P(y = 0)$ to make a prediction because it is constant with respect to $y$, but if you want to calculate probability then you need to calculate it.
\end{note}
The above equation will set the framework for \vocab{Gaussian discriminant analysis} and \vocab{Naive Bayes}.

\subsection{Gaussian Discriminant Analysis}

\begin{definition}
    The first generative learning algorithm we will observe is \vocab{Gaussian Discriminant Analysis} which can be used for continuous-valued features, like in tumor classification.
\end{definition}
We'll assume that $P(x|y)$ is distributed according to a multivariate normal distribution.

\subsection{The Multivariate Gaussian Distribution}

\begin{definition}
    The \vocab{multivariate Gaussian distribution} seeks to model multiple random variables as a Gaussian.
\end{definition}
Suppose $X$ is distributed as a multivariate Gaussian such that $X \in \RR^n$, parameterized by a mean vector $\mu\in\RR^n$ and a covariance matrix $\Sigma\in\RR^{n\times n}$ such that $\Sigma \geq 0$ is symmetric and positive definite.
\[X \sim N(\mu, \Sigma)\]
\begin{lemma}
    The PDF of $X$ is given by 
\[P(X; \mu, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(X - \mu)^T\Sigma^{-1}(X - \mu)\right)\]
where $|\Sigma| = \det\Sigma$
The expected value of $X$ is given as 
\[E(X) = \int_x xP(x; \mu, \Sigma) dx = \mu\]
\[\text{Cov}(X) = E(XX^T) - (E(X)(E(X))^T = \Sigma\]
\end{lemma}
\begin{note}
    Note that a Gaussian with zero mean and identity covariance is known as the \vocab{standard normal distribution}.
\end{note}
\begin{note}
    As the covariance $\Sigma$ becomes larger, the Gaussian becomes wider and shorter, as the distribution must adapt because the PDF must always integrate to 1. 
\end{note}
\begin{note}
    By varying $\mu$, we can shift the center of the Gaussian density.
\end{note}

\subsubsection{The Gaussian Discriminant Analysis (GDA) Model}

When the input features $x$ are continuous random variables, we can model $P(x | y)$ as a multivariate normal distribution.
\[x | y = 0 \sim N(\mu_0, \Sigma)\]
\[x | y = 1 \sim N(\mu_1, \Sigma)\]
\[y \sim \text{Bernoulli}(\phi)\]
Thus, the parameters of our GDA model are $\mu_0, \mu_1, \Sigma, \phi$. 

\begin{note}
    Note that we use the same covariance matrix $\Sigma$ for both classes, but different mean vectors.
\end{note}

Fitting these parameters to our data will define $P(x | y)$ and $P(y)$. With sufficient estimations of parameters, then we can easily ascertain a tumor as benign or malignant because we know $P(x | y = 0), P(x | y = 1), P(y = 0), P(y = 1)$.

Now let us maximize our joint likelihood.
\[L(\phi, \mu_0, \mu_1, \Sigma) = \prod_{i=1}^m P(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma) = \prod_{i=1}^mP(x^{(i) | y^{(i); \mu_0, \mu_1, \Sigma}})P(y^{(i)}; \phi)\]

\begin{note}
    The difference between $\vocab{cost functions}$ of a generative learning algorithm and a discriminative learning algorithm is that you're trying in generative, you're trying to choose parameters $\phi, \mu_0, \mu_1, \Sigma$ that maximize the joint likelihood $P(x, y; \phi, \mu_0, \mu_1, \Sigma)$ whereas in discriminative, you choose $w$ to maximize the conditional likelihood that $P(y | x; w)$. 
\end{note}

\section{Naive Bayes}

In Gaussian Discriminant Analysis, where the feature vectors $x$ were continuous and real valued, \vocab{Naive Bayes} deals with $x_i's$ that are discrete-valued, for example in the problem of spam classification. The given features will not be continuous. Let us begin by supposing we have a training dataset, now we would like to specify the feature vector using word embeddings and removing stop words. This set of words is called the vocabulary, so the dimension of $x$ is equal to the size of the vocabulary.

\begin{note}
    Note that we are using a generative model, so we wish to model $P(x | y)$. Suppose our vocabulary is 50000 words, then $x$ is a 50000 dimensional binary vector, and modelling $x$ explicitly with a multinomial distribution would yield far too many parameters.
\end{note}

Thus, we will assume that all $x_i's$ are conditionally independent given $y \implies$ and this is called the \vocab{Naive Bayes Assumption} and the resulting algorithm is the \vocab{Naive Bayes Classifier}.

\[P(x_1 | y) = P(x_1 | y, x_9)\] and this is NOT the same as 
\[P(x_1) = P(x_1 | x_9)\]
Therefore, 
\[P(x_1, \cdots, x_{50000} | y) = \prod_{i=1}^m P(x_i | y)\]
Observe that our model is parameterized by $\phi_{i | y = 1} = P(x_i = 1 | y = 1), \phi_{i | y = 0} = P(x_1 = 1 | y = 0), \phi_y = P(y = 1)$. Using joint likelihood once more, we can find the maximum likelihood estimates for the parameters below.
\[\phi_{j | y = 1} = \frac{\sum_{i=1}^m 1\{x_j^{(i)} = 1 \And y^{(i)} = 1\}}{\sum_{i=1}^m 1\{y^{(i)} = 1\}}\]
which is the fraction of the spam in which the word $j$ appears
\[\phi_{j | y = 0} = \frac{\sum_{i=1}^m 1\{x_j^{(i)} = 1 \And y^{(i)} = 0\}}{\sum_{i=1}^m 1\{y^{(i)} = 0\}}\]
which is the fraction of the nonspam in which the word $j$ appears
\[\phi_{y} = \frac{\sum_{i=1}^m 1\{y^{(i)} = 1\}}{m}\]
which is the fraction that is spam.
To make a new prediction, we seek \[P(y = 1 | x) = \frac{P(x| y = 1)P(y = 1)}{P(x)}\]
by Bayes rule which simplifies to 
\[P(y = 1 | x) = \frac{(\prod_{i=1}P(x_i | y = 1))P(y = 1)}{(\prod_{i=1}^nP(x_i | y = 1))P(y = 1) + (\prod_{i=1}^nP(x_i | y = 0))P(y=0)}\]
and then we pick whichever class is more likely.

\subsection{Laplace Smoothing}

Assume your Naive Bayes model has never seen a word before, but it must estimate $P(x_i | y)$. It will estimate 0, simply because it's never seen the word, and this would cause $\prod_{i=1}^n (P(x_i) | y)$ to be zero, and when making predictions, this would yield $\frac{0}{0}$.

\begin{note}
    It is statistically a bad idea to estimate the probability of an event to be 0 just because you have never seen it before in your finite training set. 
\end{note}

\section{Support Vector Machines}




% \section{Learning Theory}

% \subsection{The Bias/Variance Tradeoff}

% Suppose we have training data that seems to follow a shape of quadratic. Complex models such as 
% \[y = w_0 + w_1x + \cdots + w_5x^5\]
% are not always a better model compared to simpler linear models such as 
% \[y = w_0 + w_1x\]
% A 5th order polnomial may fit the training data perfectly, but it isn't necessarily a good job of predicting $y$ from $x$.

% \begin{note}
%     Both the linear model and 5th degree polynomial model will have large generalization error, although this error is different. Fitting a linear model to a nonlinear dataset will fail, no matter how much data you give it.
% \end{note}

% \begin{definition}
%     The bias of a model is the expected generalization error.
% \end{definition}
% \begin{itemize}
%     \item The linear model suffers from large bias because it will underfit the dataset. Large bias, small variance.
%     \item THe 5th order polynomial has a large risk of overfitting and learning patterns in the training dataset that are not representative of the wider pattern of the relationship between $x$ and $y$. Again, this model will have a large generalization error. In this case, we say the model has large variance. Small bias, large variance.
% \end{itemize}

% \begin{note}
%     Noet that the definitions of bias and variance are informal. They are straightforward to define for linear regression, but not for classification.
% \end{note}

% \subsubsection{Preliminaries}

% Some questions to think about before moving forward:
% \begin{itemize}
%     \item What are good rules of thumb to best apply learning algorithms in different settings?
%     \item Can we formalize our discussion of the bias and variance tradeoff?
%     \item Can we relate error on the training set to generalization error?
% \end{itemize}

% \begin{lemma}
%     \[\mathbb{P}(A_1 \cup \cdots \cup A_k) \leq \mathbb{P}(A_1) + \cdots + \mathbb{P}(A_k)\]
% \end{lemma}
% \begin{lemma}
%     Hoeffding inequality: Let $Z_1, \cdots, Z_m$ be $m$ IID random variables drawn from a Bernoulli distribution such that 
%     \[\mathbb{P}(Z_i = 1) = \phi, \mathbb{P}(Z_i = 0) = 1 - \phi\]
%     Then let \[\hat{\phi} = \frac{1}{m}\sum_{i=1}^m Z_i\] be the mean of these IID random variables, and let $\gamma \in \RR^{>0}$ be fixed. Then, 
%     \[\mathbb{P}(|\phi-\hat \phi)| > \gamma) \leq 2 \exp(-2\gamma^2m)\]
%     In learning theory, this is called the Chernoff bound, and it says that if we take the average of the $m$ Bernoulli rvs to be our estimate of $\phi$, then the probability of our being far from the true value is small given that $m$ is large. 
% \end{lemma}

% Using these two lemmas will help us prove some of the most important results in learning theory. For simplicity, we will model this example as as binary classification problem such that $y \in \{0, 1\}$, but these ideas will generalize to regression, multiclass classification, etc. Suppose we have a training set $S = \{(x^{(i)}, y^{(i)}; i = 1 \cdots m)$ of size $m$, and the training examples are drawn IID from a probability distribution $D$. The hypothesis $h$ we call the training error which is 
% \[\hat \epsilon(h) = \frac{1}{m}\sum_{i=1}^m 1 \{h(x^{(i)}) \neq y^{(i)}\}\]
% and note that this is just the fraction of training examples that $h$ misclassifies.

% We define the generalization error to be 
% \[\epsilon(h) = P_{(x,y) \sim D} (h(x) \neq y)\] such that this is the probability that if we now draw a new example $(x,y)$ from the distribution $D$, $h$ will misclassify it.

% \begin{note}
%     Note that we are under the assumption that the training data and testing data are of the same probability distribution, and this is a crucial assumption.
% \end{note}

% Consider the setting of linear classification such that our hypothesis function is 
% \[h(x) = 1\{w^Tx \geq 0\}\]

% \begin{definition}
% What's a reasonable way of fitting $w$? One approach is to miniize the training error and pick
% \[\hat w = \text{argmin}_{w}\hat\epsilon(h_w)\] 
% We call this process $\vocab{empirical risk minimization (ERM)}$, such that the resulting hypothesis output by the learning algorithm is $\hat h = h_{\hat w}$.    
% \end{definition}

% If we wish to generalize this notion further and abstract away the specific parameterization of hypotheses, we define the hypothesis class $H$ used by a learning algorithm such that 
% \[H = \{h: h(x) = 1 \{w^Tx\geq 0\}, w \in \RR^{n+1}\]
% where $H$ is the set of all classifiers over the domain of inputs. Now, ERM can be thought of as the minization over the class of functions $H$ in which the learning algorithm picks the hypothesis 
% \[\hat h = \text{argmin}_{h \in H}\hat \epsilon(h)\]

\end{document}
